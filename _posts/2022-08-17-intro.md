---
layout: post
title: "Sensitivity@Specificity formulation"
excerpt: "maximising sensitivity at a set specificity using a novel ranking loss equation"
tags: [ranking loss, AUROC]
comments: true
---

## Hello



$\alpha$ : the target specificity

$b$ : the threshold at which the classification should be made

$|Y^{-}| = \text{total number of negatives}$

$\mathcal{L}^{-} = \text{false positives}$

$|Y^{+}| = \text{total number of positives}$

$\mathcal{L}^{+} = |Y^{+}| - \text{tp (all positives covered by the model)}$


$\max \dfrac{tp(f)}{|Y^{+}|} (Sensitivity)$

$\text{st. TNR(Specificity)} > 1-\alpha$

is equivalent to 

$\max \dfrac{tp(f)}{|Y^{+}|}$

$st. \dfrac{fp(f)}{|Y^{-}|} < \alpha \leftrightarrow FPR<\alpha$

Given the previous definitions:
 
it is known that
 

$tp(f)\geq{tp^{l}(f)}$=$|Y^{+}}|-\mathcal{L}^{+}$


$fp(f)\leq{fp^{u}(f)}=\mathcal{L}^{-}$
 
   
substitute a new objective:
   
$^{\max}_{f}  \dfrac {tp_{l}(f)} {{|Y^{+}|}} (TPR) \leftrightarrow ^{\max}_{f}  \dfrac {|Y^{+}|-\mathcal{L^{+}}} {{|Y^{+}|}} = 1-\dfrac{\mathcal{L}^{+}}{|Y^{+}|}$
  
$st.  \dfrac {fp} {{|Y^{-}|}} (FPR) < \dfrac{fp^{u}}{|Y^{+}| }< \alpha \leftrightarrow st.  \dfrac {\mathcal{L}^{-}} {{|Y^{-}|}} < \alpha \leftrightarrow \mathcal{L}^{-} < |Y^{-}|$



Hence the objective is:

$\min \dfrac {\mathcal{L}^{+}} {{|Y^{+}|}}$
   
   
$st.  \mathcal{L}^{-} -  \alpha {{|Y^{-}|}} < 0$


and the loss function can be calculated by

$L = \dfrac {\mathcal{L}^{+}}{{|Y^{+}|}} + \gamma (\mathcal{L}^{-} - \alpha {|Y^{-}|})\] \[\leftrightarrow L = \mathcal{L}^{+} + \gamma \mathcal{L}^{-}|Y^{-}| - \gamma{|Y^{+}|}{|Y^{-}|\alpha}$

This loss function can replace BCE loss to adjust a model's performance to achieve the objective set out in this work, maximising Sensitivity at a set Specificity.






## Ranking loss

Binary Cross Entropy to to Log Ratio


$-\{\sum y_{i}\log p (p(y_{i}=1|x_{i})) + (1-y_{i}\log p(y_{i}=0|x_{i})\}$

represents approximately $p(\centerdot |x), \text{using a neural net } ~ \hat{p}(|x)$

maximum likelihood
$\Pi p(y=i|x)^{y}p(y=0|x)^{1-y}\]

$log\Pi (...) =\sum \log (p(y=1|x)^{y} p(y=0|x)^{1-y}$

$=\sum y \log (p(y|x)^{y} + (1-y) \log (p(y=0|x))$

$= \{ - \sum_{x \in y^{+}} \log(p(y=1|x)) + \sum \log(1-p(y=1|x)) \}$

$p(y=1|x) \text{ as }p$

$- \{ \sum \log p^{+} + \sum \log (1-p^{-}) \}$

$\text{max}\log(1-p) \leftrightarrow 1-p$
$\text{eq min } p-p(y=1|x \in y^{-})$
$text{min} \log(p^{-})\]

$\sum\log(p^{+}) - \sum\log (p^{-})$

$= log(\dfrac{p^{+}}{p^{-}})$

Non-Decomposable (requires a memory bank or large batch size)

$log(\dfrac{p^{+}}{p^{-}}) \approx AUC$

$\text{max}(log^{+} - log^{-})$

$\text{max}(log^{-} - log^{+})$

$\text{max}(log(\dfrac{p^{-}}{p^{+}}))$


$\text{if } p^{-} < p^{+} -> \text{all correct} = 0$

Decomposable

$log(\dfrac{threshold}{p^{+}})$
$log(\dfrac{p^{-}}{threshold})$

$\dfrac{p^{-}}{threshold}<1log<0$

$$\dfrac{threshold>p^{+}}{threshold<p^{-}}




$$threshold<p^{+}\rightarrow\dfrac{threshold}{p^{+}}<1, log(...)<0


  
